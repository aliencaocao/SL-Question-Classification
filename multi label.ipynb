{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0-rc0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3080 Ti, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model, models, mixed_precision\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_text  # must import even if it is not used, else will have error\n",
    "import tensorflow_hub as hub\n",
    "import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "\n",
    "# all_files = glob.glob(\"*MA.csv\")\n",
    "# li = []\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0)\n",
    "#     li.append(df)\n",
    "#\n",
    "# df = pd.concat(li, axis=0, ignore_index=True)\n",
    "# df.dropna(inplace=True)\n",
    "# df = df[['chapter_name', 'qns']]\n",
    "# df.to_csv('all_qns.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('all_qns_multi_label.csv')\n",
    "labels = df['chapter_name'].str.lower().tolist()\n",
    "labels = [[*i.split(',')] for i in labels]\n",
    "classes = []\n",
    "for i in labels:\n",
    "    for j in i:\n",
    "        if j not in classes:\n",
    "            classes.append(j)\n",
    "classes_index = {v: i for i, v in enumerate(classes)}\n",
    "def multi_hot_encode(label):\n",
    "    label_encoded = [classes_index[x] for x in label]\n",
    "    label_encoded = tf.reduce_max(tf.one_hot(label_encoded, len(classes)), axis=0)\n",
    "    return label_encoded\n",
    "\n",
    "labeled_df = pd.DataFrame()\n",
    "labels = [multi_hot_encode(i) for i in labels]\n",
    "\n",
    "def clean(qns):\n",
    "    import re\n",
    "    qns = re.sub(r'[^\\x00-\\x7F]+', ' ', qns)  # clean unicode stuff\n",
    "    qns = re.sub(r'\\d+', ' 0 ', qns)  # replace all numbers with 0\n",
    "    qns = re.sub(r'_+', ' _ ', qns)  # replace all underscores with single underscore\n",
    "    qns = qns.split('(Note to students')[0].split('Notes to student')[0].split('Note to students')[0].split('Note to student')[0].split('Notes to students')[0].split('(Separate ')[0]  # strip hints/notes\n",
    "    qns = qns.strip()\n",
    "    return qns\n",
    "\n",
    "\n",
    "qns = df['qns'].apply(clean)\n",
    "\n",
    "labeled_df['label'] = pd.Series(labels)\n",
    "labeled_df['qns'] = qns\n",
    "labeled_df.dropna(inplace=True)\n",
    "labeled_df.to_csv('multi_labeled_df.csv', index=False)\n",
    "\n",
    "# shuffle the dataset\n",
    "X, Y = shuffle(qns, labels)\n",
    "\n",
    "\n",
    "# labeled_df['label'] = pd.Series(labels)\n",
    "# labeled_df['qns'] = df['qns'].apply(clean)\n",
    "# labeled_df.dropna(inplace=True)\n",
    "# labeled_df.to_csv('multi_labeled_df.csv', index=False)\n",
    "# labeled_df = labeled_df.sample(frac=1).reset_index(drop=True)  # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# an accuracy metrics that excludes negative classes for each sample\n",
    "def positive_accuracy(y_true, y_pred):\n",
    "    thresh = 0.5\n",
    "    positive = y_true * y_pred\n",
    "    total_score = tf.reduce_sum(tf.cast(tf.greater_equal(positive, thresh), tf.float32))\n",
    "    acc = total_score / tf.reduce_sum(y_true)\n",
    "    return acc\n",
    "\n",
    "# weighted BCE\n",
    "def weighted_binary_crossentropy(pos_weight=1.):\n",
    "\n",
    "    # y_pred is the raw output of the logits layer\n",
    "    def _weighted_binary_crossentropy(y_true, y_pred):\n",
    "        return tf.keras.backend.mean(tf.nn.weighted_cross_entropy_with_logits(labels=y_true, logits=y_pred, pos_weight=pos_weight), axis=-1)\n",
    "    \n",
    "    return _weighted_binary_crossentropy\n",
    "\n",
    "loss = weighted_binary_crossentropy(pos_weight=15.)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1,\n",
    "                                     mode='auto', baseline=None, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=12, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessing_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer('https://tfhub.dev/google/experts/bert/wiki_books/qnli/2', trainable=False)\n",
    "outputs = encoder(encoder_inputs)\n",
    "x = outputs['pooled_output']\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "xOut = Dense(len(classes), activation=None)(x)\n",
    "model = Model(text_input, xOut)\n",
    "model.compile(loss=loss, optimizer=opt, metrics=['accuracy', positive_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " keras_layer (KerasLayer)       {'input_type_ids':   0           ['input_1[0][0]']                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128)}                                                          \n",
      "                                                                                                  \n",
      " keras_layer_1 (KerasLayer)     {'encoder_outputs':  109482241   ['keras_layer[0][0]',            \n",
      "                                 [(None, 128, 768),               'keras_layer[0][1]',            \n",
      "                                 (None, 128, 768),                'keras_layer[0][2]']            \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)],                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768)}                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          393728      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 36)           18468       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,157,093\n",
      "Trainable params: 674,852\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 39s 784ms/step - loss: 0.6766 - accuracy: 0.2159 - positive_accuracy: 0.4706 - val_loss: 0.5330 - val_accuracy: 0.2908 - val_positive_accuracy: 0.5937 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 33s 924ms/step - loss: 0.5172 - accuracy: 0.3145 - positive_accuracy: 0.6168 - val_loss: 0.4308 - val_accuracy: 0.4603 - val_positive_accuracy: 0.7283 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 23s 648ms/step - loss: 0.4308 - accuracy: 0.3903 - positive_accuracy: 0.7015 - val_loss: 0.3663 - val_accuracy: 0.4873 - val_positive_accuracy: 0.7930 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 24s 663ms/step - loss: 0.3600 - accuracy: 0.4666 - positive_accuracy: 0.7684 - val_loss: 0.3110 - val_accuracy: 0.5109 - val_positive_accuracy: 0.8682 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 24s 676ms/step - loss: 0.3171 - accuracy: 0.5203 - positive_accuracy: 0.8089 - val_loss: 0.2846 - val_accuracy: 0.5886 - val_positive_accuracy: 0.8617 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 24s 666ms/step - loss: 0.2844 - accuracy: 0.5496 - positive_accuracy: 0.8354 - val_loss: 0.2460 - val_accuracy: 0.6017 - val_positive_accuracy: 0.9007 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 23s 654ms/step - loss: 0.2567 - accuracy: 0.5852 - positive_accuracy: 0.8528 - val_loss: 0.2399 - val_accuracy: 0.6533 - val_positive_accuracy: 0.9123 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 23s 648ms/step - loss: 0.2403 - accuracy: 0.5994 - positive_accuracy: 0.8641 - val_loss: 0.2308 - val_accuracy: 0.6341 - val_positive_accuracy: 0.9034 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 23s 639ms/step - loss: 0.2241 - accuracy: 0.6134 - positive_accuracy: 0.8786 - val_loss: 0.2155 - val_accuracy: 0.6917 - val_positive_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 23s 635ms/step - loss: 0.2048 - accuracy: 0.6528 - positive_accuracy: 0.8884 - val_loss: 0.2103 - val_accuracy: 0.6725 - val_positive_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 23s 635ms/step - loss: 0.2023 - accuracy: 0.6447 - positive_accuracy: 0.8877 - val_loss: 0.2027 - val_accuracy: 0.6445 - val_positive_accuracy: 0.9117 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 23s 647ms/step - loss: 0.1903 - accuracy: 0.6523 - positive_accuracy: 0.8987 - val_loss: 0.1914 - val_accuracy: 0.6926 - val_positive_accuracy: 0.9157 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 23s 638ms/step - loss: 0.1844 - accuracy: 0.6753 - positive_accuracy: 0.9024 - val_loss: 0.1834 - val_accuracy: 0.7162 - val_positive_accuracy: 0.9106 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 23s 639ms/step - loss: 0.1725 - accuracy: 0.6917 - positive_accuracy: 0.9109 - val_loss: 0.1854 - val_accuracy: 0.6699 - val_positive_accuracy: 0.9184 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.1738 - accuracy: 0.6702 - positive_accuracy: 0.9088 - val_loss: 0.1867 - val_accuracy: 0.6638 - val_positive_accuracy: 0.9160 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 23s 645ms/step - loss: 0.1621 - accuracy: 0.6879 - positive_accuracy: 0.9175 - val_loss: 0.1776 - val_accuracy: 0.7118 - val_positive_accuracy: 0.9209 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 23s 651ms/step - loss: 0.1511 - accuracy: 0.7041 - positive_accuracy: 0.9240 - val_loss: 0.1771 - val_accuracy: 0.7214 - val_positive_accuracy: 0.9160 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 23s 645ms/step - loss: 0.1499 - accuracy: 0.7065 - positive_accuracy: 0.9259 - val_loss: 0.1795 - val_accuracy: 0.6900 - val_positive_accuracy: 0.9240 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.1423 - accuracy: 0.7096 - positive_accuracy: 0.9272 - val_loss: 0.1732 - val_accuracy: 0.7074 - val_positive_accuracy: 0.9138 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 24s 657ms/step - loss: 0.1464 - accuracy: 0.7041 - positive_accuracy: 0.9285 - val_loss: 0.1798 - val_accuracy: 0.6795 - val_positive_accuracy: 0.9298 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 24s 655ms/step - loss: 0.1425 - accuracy: 0.7063 - positive_accuracy: 0.9279 - val_loss: 0.1730 - val_accuracy: 0.6830 - val_positive_accuracy: 0.9137 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 22s 627ms/step - loss: 0.1362 - accuracy: 0.7096 - positive_accuracy: 0.9311 - val_loss: 0.1725 - val_accuracy: 0.7039 - val_positive_accuracy: 0.9153 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.1338 - accuracy: 0.7139 - positive_accuracy: 0.9316 - val_loss: 0.1735 - val_accuracy: 0.7100 - val_positive_accuracy: 0.9147 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 23s 642ms/step - loss: 0.1282 - accuracy: 0.7181 - positive_accuracy: 0.9407 - val_loss: 0.1724 - val_accuracy: 0.6917 - val_positive_accuracy: 0.9113 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 23s 642ms/step - loss: 0.1216 - accuracy: 0.7365 - positive_accuracy: 0.9445 - val_loss: 0.1659 - val_accuracy: 0.7371 - val_positive_accuracy: 0.9131 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.1241 - accuracy: 0.7408 - positive_accuracy: 0.9377 - val_loss: 0.1764 - val_accuracy: 0.7083 - val_positive_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.1211 - accuracy: 0.7378 - positive_accuracy: 0.9411 - val_loss: 0.1695 - val_accuracy: 0.7231 - val_positive_accuracy: 0.9126 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 23s 641ms/step - loss: 0.1195 - accuracy: 0.7432 - positive_accuracy: 0.9411 - val_loss: 0.1628 - val_accuracy: 0.7415 - val_positive_accuracy: 0.9199 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 23s 650ms/step - loss: 0.1183 - accuracy: 0.7448 - positive_accuracy: 0.9452 - val_loss: 0.1683 - val_accuracy: 0.7441 - val_positive_accuracy: 0.9151 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 23s 640ms/step - loss: 0.1111 - accuracy: 0.7533 - positive_accuracy: 0.9488 - val_loss: 0.1661 - val_accuracy: 0.7231 - val_positive_accuracy: 0.9257 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.1077 - accuracy: 0.7563 - positive_accuracy: 0.9525 - val_loss: 0.1727 - val_accuracy: 0.7354 - val_positive_accuracy: 0.9212 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 23s 639ms/step - loss: 0.1119 - accuracy: 0.7496 - positive_accuracy: 0.9503 - val_loss: 0.1818 - val_accuracy: 0.7328 - val_positive_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 23s 639ms/step - loss: 0.1103 - accuracy: 0.7456 - positive_accuracy: 0.9476 - val_loss: 0.1712 - val_accuracy: 0.7301 - val_positive_accuracy: 0.9142 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.1024 - accuracy: 0.7640 - positive_accuracy: 0.9533 - val_loss: 0.1695 - val_accuracy: 0.6996 - val_positive_accuracy: 0.9133 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 22s 623ms/step - loss: 0.1060 - accuracy: 0.7426 - positive_accuracy: 0.9516 - val_loss: 0.1779 - val_accuracy: 0.7022 - val_positive_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.1060 - accuracy: 0.7491 - positive_accuracy: 0.9505 - val_loss: 0.1690 - val_accuracy: 0.7284 - val_positive_accuracy: 0.9137 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 23s 630ms/step - loss: 0.0984 - accuracy: 0.7635 - positive_accuracy: 0.9551 - val_loss: 0.1892 - val_accuracy: 0.7214 - val_positive_accuracy: 0.9008 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 23s 635ms/step - loss: 0.1006 - accuracy: 0.7559 - positive_accuracy: 0.9524 - val_loss: 0.1717 - val_accuracy: 0.7362 - val_positive_accuracy: 0.9072 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 23s 631ms/step - loss: 0.1005 - accuracy: 0.7710 - positive_accuracy: 0.9553 - val_loss: 0.1721 - val_accuracy: 0.7118 - val_positive_accuracy: 0.9121 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.0977 - accuracy: 0.7771 - positive_accuracy: 0.9539 - val_loss: 0.1701 - val_accuracy: 0.7467 - val_positive_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 23s 638ms/step - loss: 0.0969 - accuracy: 0.7764 - positive_accuracy: 0.9558 - val_loss: 0.1747 - val_accuracy: 0.7231 - val_positive_accuracy: 0.8994 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 0.0950 - accuracy: 0.7646 - positive_accuracy: 0.9561 - val_loss: 0.1809 - val_accuracy: 0.7493 - val_positive_accuracy: 0.9009 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 0.0913 - accuracy: 0.7751 - positive_accuracy: 0.9573 - val_loss: 0.1825 - val_accuracy: 0.7380 - val_positive_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 0.0905 - accuracy: 0.7797 - positive_accuracy: 0.9622 - val_loss: 0.1884 - val_accuracy: 0.7397 - val_positive_accuracy: 0.8967 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.0911 - accuracy: 0.7723 - positive_accuracy: 0.9593 - val_loss: 0.1789 - val_accuracy: 0.7214 - val_positive_accuracy: 0.9040 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 23s 637ms/step - loss: 0.0873 - accuracy: 0.7819 - positive_accuracy: 0.9595 - val_loss: 0.1909 - val_accuracy: 0.7441 - val_positive_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 23s 647ms/step - loss: 0.0893 - accuracy: 0.7874 - positive_accuracy: 0.9604 - val_loss: 0.1835 - val_accuracy: 0.7231 - val_positive_accuracy: 0.9033 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 23s 640ms/step - loss: 0.0877 - accuracy: 0.7828 - positive_accuracy: 0.9601 - val_loss: 0.1877 - val_accuracy: 0.7301 - val_positive_accuracy: 0.9047 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 23s 629ms/step - loss: 0.0853 - accuracy: 0.7821 - positive_accuracy: 0.9619 - val_loss: 0.1879 - val_accuracy: 0.7432 - val_positive_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 23s 639ms/step - loss: 0.0863 - accuracy: 0.7804 - positive_accuracy: 0.9617 - val_loss: 0.1785 - val_accuracy: 0.7642 - val_positive_accuracy: 0.9019 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 23s 634ms/step - loss: 0.0839 - accuracy: 0.7793 - positive_accuracy: 0.9629 - val_loss: 0.1903 - val_accuracy: 0.7188 - val_positive_accuracy: 0.9104 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 23s 641ms/step - loss: 0.0861 - accuracy: 0.7817 - positive_accuracy: 0.9610 - val_loss: 0.1874 - val_accuracy: 0.7284 - val_positive_accuracy: 0.8958 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 23s 641ms/step - loss: 0.0791 - accuracy: 0.7961 - positive_accuracy: 0.9646 - val_loss: 0.2053 - val_accuracy: 0.7651 - val_positive_accuracy: 0.9050 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 23s 644ms/step - loss: 0.0809 - accuracy: 0.7963 - positive_accuracy: 0.9633 - val_loss: 0.1909 - val_accuracy: 0.7511 - val_positive_accuracy: 0.9156 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 23s 643ms/step - loss: 0.0787 - accuracy: 0.7922 - positive_accuracy: 0.9656 - val_loss: 0.1969 - val_accuracy: 0.7170 - val_positive_accuracy: 0.8881 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 23s 645ms/step - loss: 0.0799 - accuracy: 0.7887 - positive_accuracy: 0.9686 - val_loss: 0.1991 - val_accuracy: 0.7188 - val_positive_accuracy: 0.8980 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 23s 636ms/step - loss: 0.0777 - accuracy: 0.7998 - positive_accuracy: 0.9630 - val_loss: 0.1994 - val_accuracy: 0.7214 - val_positive_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 23s 634ms/step - loss: 0.0765 - accuracy: 0.7893 - positive_accuracy: 0.9667 - val_loss: 0.2072 - val_accuracy: 0.7231 - val_positive_accuracy: 0.8823 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 23s 633ms/step - loss: 0.0768 - accuracy: 0.7915 - positive_accuracy: 0.9661 - val_loss: 0.1840 - val_accuracy: 0.7310 - val_positive_accuracy: 0.8993 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 23s 646ms/step - loss: 0.0709 - accuracy: 0.7974 - positive_accuracy: 0.9704 - val_loss: 0.2006 - val_accuracy: 0.7380 - val_positive_accuracy: 0.8861 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 23s 654ms/step - loss: 0.0722 - accuracy: 0.8118 - positive_accuracy: 0.9683 - val_loss: 0.1942 - val_accuracy: 0.7694 - val_positive_accuracy: 0.9011 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 23s 656ms/step - loss: 0.0753 - accuracy: 0.8068 - positive_accuracy: 0.9666 - val_loss: 0.2028 - val_accuracy: 0.7406 - val_positive_accuracy: 0.9040 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 23s 648ms/step - loss: 0.0761 - accuracy: 0.7902 - positive_accuracy: 0.9681 - val_loss: 0.2031 - val_accuracy: 0.7511 - val_positive_accuracy: 0.8873 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 24s 662ms/step - loss: 0.0754 - accuracy: 0.8022 - positive_accuracy: 0.9651 - val_loss: 0.1889 - val_accuracy: 0.7301 - val_positive_accuracy: 0.9085 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 24s 669ms/step - loss: 0.0742 - accuracy: 0.8018 - positive_accuracy: 0.9692 - val_loss: 0.1917 - val_accuracy: 0.7389 - val_positive_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 24s 676ms/step - loss: 0.0700 - accuracy: 0.8083 - positive_accuracy: 0.9709 - val_loss: 0.2085 - val_accuracy: 0.7642 - val_positive_accuracy: 0.8824 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 24s 680ms/step - loss: 0.0710 - accuracy: 0.8116 - positive_accuracy: 0.9710 - val_loss: 0.2011 - val_accuracy: 0.7441 - val_positive_accuracy: 0.8940 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 23s 648ms/step - loss: 0.0738 - accuracy: 0.8020 - positive_accuracy: 0.9669 - val_loss: 0.2139 - val_accuracy: 0.7380 - val_positive_accuracy: 0.8940 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 23s 649ms/step - loss: 0.0719 - accuracy: 0.8064 - positive_accuracy: 0.9701 - val_loss: 0.1997 - val_accuracy: 0.7555 - val_positive_accuracy: 0.8925 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 23s 650ms/step - loss: 0.0725 - accuracy: 0.7987 - positive_accuracy: 0.9675 - val_loss: 0.2082 - val_accuracy: 0.7319 - val_positive_accuracy: 0.8793 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 23s 648ms/step - loss: 0.0729 - accuracy: 0.8031 - positive_accuracy: 0.9691 - val_loss: 0.2044 - val_accuracy: 0.7397 - val_positive_accuracy: 0.9000 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 23s 651ms/step - loss: 0.0725 - accuracy: 0.8116 - positive_accuracy: 0.9698 - val_loss: 0.2057 - val_accuracy: 0.7406 - val_positive_accuracy: 0.8963 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.7976 - positive_accuracy: 0.9690\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "36/36 [==============================] - 24s 663ms/step - loss: 0.0682 - accuracy: 0.7976 - positive_accuracy: 0.9690 - val_loss: 0.2201 - val_accuracy: 0.7109 - val_positive_accuracy: 0.8919 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 23s 649ms/step - loss: 0.0633 - accuracy: 0.8114 - positive_accuracy: 0.9758 - val_loss: 0.2145 - val_accuracy: 0.7223 - val_positive_accuracy: 0.8919 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 23s 629ms/step - loss: 0.0583 - accuracy: 0.8215 - positive_accuracy: 0.9807 - val_loss: 0.2174 - val_accuracy: 0.7275 - val_positive_accuracy: 0.8856 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.8289 - positive_accuracy: 0.9767Restoring model weights from the end of the best epoch: 61.\n",
      "36/36 [==============================] - 23s 628ms/step - loss: 0.0578 - accuracy: 0.8289 - positive_accuracy: 0.9767 - val_loss: 0.2168 - val_accuracy: 0.7266 - val_positive_accuracy: 0.8855 - lr: 1.0000e-04\n",
      "Epoch 76: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x25d2e60bf10>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, show_dtype=True, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "model.summary()\n",
    "model.fit(np.array(X), np.array(Y), batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=1, use_multiprocessing=True, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('multi_label_77.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predicted: shapes with confidence 96.438%\n",
      "Class predicted: geometry with confidence 98.562%\n",
      "Class predicted: logical reasoning with confidence 62.844%\n",
      "Class predicted: triangles with confidence 64.062%\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'counting': '0.000%',\n 'patterns': '0.000%',\n 'place values': '0.000%',\n 'addition': '0.000%',\n 'subtraction': '0.000%',\n 'comparing': '0.000%',\n 'estimation': '0.000%',\n 'shapes': '96.438%',\n 'geometry': '98.562%',\n 'spatial sense': '0.040%',\n 'data graphs': '0.000%',\n 'measurement': '0.049%',\n 'money': '0.000%',\n 'probability and statistics': '0.374%',\n 'ordering': '0.000%',\n 'classifying': '0.000%',\n 'time': '0.000%',\n 'mixed operations': '0.000%',\n 'multiplication': '0.000%',\n 'division': '0.000%',\n 'names of numbers': '0.000%',\n 'estimation and rounding': '0.000%',\n 'logical reasoning': '62.844%',\n 'fractions': '0.128%',\n 'learning to interprete': '0.113%',\n 'whole numbers': '0.024%',\n 'decimals': '0.000%',\n 'ratio': '0.664%',\n 'challenge': '0.000%',\n 'average': '0.010%',\n 'percentage': '0.010%',\n 'area & perimeter': '0.047%',\n 'volume': '22.266%',\n 'triangles': '64.062%',\n 'speed': '0.003%',\n 'circles': '13.188%'}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'The height of an isosceles triangle with a base length of 8cm is 3cm. What is the perimeter of a similar triangle with base 4cm?'\n",
    "question = clean(question)\n",
    "sigmoid_out = Activation('sigmoid')(model(tf.constant([question])).numpy()[0])\n",
    "confidence = tf.where(tf.greater_equal(sigmoid_out, 0.5)).numpy()\n",
    "class_predicted = [classes[i[0]] for i in confidence]\n",
    "for class_, conf in zip(class_predicted, confidence):\n",
    "    print(f'Class predicted: {class_} with confidence {sigmoid_out[conf[0]]*100:.3f}%')\n",
    "conf_list = {}\n",
    "for i in range(len(classes)):\n",
    "    conf_list.update({classes[i]: f'{sigmoid_out[i]*100:.3f}%'})\n",
    "\n",
    "conf_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}